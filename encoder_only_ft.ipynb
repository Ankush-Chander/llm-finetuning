{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIrQ9wZmJtm9"
   },
   "source": [
    "# Processing the data (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eihByiSVJtnA"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcOg2v-AJtnB",
    "outputId": "832cd899-c9bd-4e84-ae07-8da1509f0641"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate transformers[sentencepiece]\n",
    "#!pip install 'accelerate>=0.26.0'\n",
    "#!pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Tokenizer is used to encode and decode text.  \n",
    "encode: token => token_id  \n",
    "decode: token_id => token  \n",
    "\n",
    "\n",
    "**Vocabulary size**: number of unique tokens in the vocabulary.  \n",
    "**Special tokens**: special tokens that are used in the model.\n",
    "- UNK token: token used to represent unknown tokens.\n",
    "- SEP token: token used to separate input ids into different sequences.\n",
    "- PAD token: token used to pad sequences.\n",
    "- CLS token: token used to start sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.vocab_size: 30522\n",
      "tokenizer.unk_token: [UNK] = 100\n",
      "zyxw: [100]\n",
      "tokenizer.sep_token: [SEP] = 102\n",
      "tokenizer.pad_token: [PAD] = 0\n",
      "tokenizer.cls_token: [CLS] = 101\n",
      "tokenizer.mask_token: [MASK]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "print(f\"tokenizer.vocab_size: {tokenizer.vocab_size}\")\n",
    "# special tokens\n",
    "print(f\"tokenizer.unk_token: {tokenizer.unk_token} = {tokenizer.unk_token_id}\")\n",
    "print(f\"zyxw: {tokenizer.convert_tokens_to_ids(['zyxw'])}\")\n",
    "\n",
    "print(f\"tokenizer.sep_token: {tokenizer.sep_token} = {tokenizer.sep_token_id}\")\n",
    "print(f\"tokenizer.pad_token: {tokenizer.pad_token} = {tokenizer.pad_token_id}\")\n",
    "print(f\"tokenizer.cls_token: {tokenizer.cls_token} = {tokenizer.cls_token_id}\")\n",
    "print(f\"tokenizer.mask_token: {tokenizer.mask_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use for mask filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.input_ids.shape: torch.Size([1, 9])\n",
      "mask_token_index: tensor([6])\n",
      "torch.Size([1, 9, 30522])\n",
      "sentence: the capital of India is [MASK].\n",
      "   >>>1: mumbai\n",
      "   >>>2: delhi\n",
      "   >>>3: pune\n",
      "   >>>4: hyderabad\n",
      "   >>>5: bangalore\n",
      "inputs.input_ids.shape: torch.Size([1, 10])\n",
      "mask_token_index: tensor([4])\n",
      "torch.Size([1, 10, 30522])\n",
      "sentence: the capital of [MASK] is New Delhi.\n",
      "   >>>1: india\n",
      "   >>>2: delhi\n",
      "   >>>3: district\n",
      "   >>>4: haryana\n",
      "   >>>5: state\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "\n",
    "def fill_mask(sentence, topk=5):\n",
    "    \"\"\"\n",
    "    Print topk candidates for the masked token in the sentence.\n",
    "    \"\"\"\n",
    "    if \"[MASK]\" not in sentence:\n",
    "        raise ValueError(\"Input sentence must contain [MASK] token.\")\n",
    "    \n",
    "    # Tokenize input and get tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    # input_ids, token_type_ids, attention_mask\n",
    "    print(f\"inputs.input_ids.shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "    # findout the index of tokens which is masked\n",
    "    mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "    print(f\"mask_token_index: {mask_token_index}\")\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    print(outputs.logits.shape)\n",
    "    \n",
    "    # Extract the logits for the masked token\n",
    "    mask_logits = outputs.logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    topk_ids = torch.topk(mask_logits, topk, dim=1).indices[0].tolist()\n",
    "    topk_tokens = [tokenizer.decode([token_id]) for token_id in topk_ids]\n",
    "\n",
    "    print(f\"sentence: {sentence}\")\n",
    "    # Print predictions\n",
    "    for i, token in enumerate(topk_tokens, 1):\n",
    "        print(f\"   >>>{i}: {token}\")\n",
    "\n",
    "fill_mask(\"the capital of India is [MASK].\")\n",
    "fill_mask(\"the capital of [MASK] is New Delhi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/net_loss_optimizer.png\" alt=\"drawing\" width=\"512\"/>  \n",
    "\n",
    "Image credit: [Deep learning with Python](https://www.manning.com/books/deep-learning-with-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finetune with toy dataset using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========before finetuning======\n",
      "torch.Size([3, 16])\n",
      "I've been waiting for a HuggingFace course my whole life.: tensor([0.3098, 0.6902])\n",
      "This course is amazing!: tensor([0.3245, 0.6755])\n",
      "The movie was horrible.: tensor([0.2866, 0.7134])\n",
      "torch.Size([3, 9])\n",
      "The hotel was not that good.: tensor([0.2710, 0.7290])\n",
      "I hate this so much!: tensor([0.3006, 0.6994])\n",
      "The movie was great.: tensor([0.2659, 0.7341])\n",
      "=========after finetuning======\n",
      "training data:\n",
      "torch.Size([3, 16])\n",
      "I've been waiting for a HuggingFace course my whole life.: tensor([0.2600, 0.7400])\n",
      "This course is amazing!: tensor([0.3101, 0.6899])\n",
      "The movie was horrible.: tensor([0.4870, 0.5130])\n",
      "test data:\n",
      "torch.Size([3, 9])\n",
      "The hotel was not that good.: tensor([0.3294, 0.6706])\n",
      "I hate this so much!: tensor([0.3063, 0.6937])\n",
      "The movie was great.: tensor([0.4391, 0.5609])\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "class_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)%%!\n",
    "\n",
    "\n",
    "# classify few sentences with the model\n",
    "training_sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "    \"The movie was horrible.\",\n",
    "]\n",
    "\n",
    "test_sentences = [\n",
    "    \"The hotel was not that good.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"The movie was great.\",\n",
    "]\n",
    "\n",
    "\n",
    "def predict(sentences:str):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    print(inputs.input_ids.shape)\n",
    "    with torch.no_grad():\n",
    "        outputs = class_model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    for sentence, pred in zip(sentences, predictions):\n",
    "        print(f\"{sentence}: {pred}\")\n",
    "\n",
    "\n",
    "print(f\"=========before finetuning======\")\n",
    "predict(training_sentences)\n",
    "predict(test_sentences)\n",
    "\n",
    "# prepare toy classification dataset out of above sentences\n",
    "inputs = tokenizer(training_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs[\"labels\"] = torch.tensor([1, 1, 0])\n",
    "\n",
    "\n",
    "# finetune the model\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = torch.optim.Adam(class_model.parameters(), lr=5e-5)\n",
    "# forward pass to calculate loss\n",
    "loss = class_model(**inputs).loss\n",
    "# backward pass to calculate gradients\n",
    "loss.backward()\n",
    "# update model weights\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "print(f\"=========after finetuning======\")\n",
    "print(f\"training data:\")\n",
    "predict(training_sentences)\n",
    "\n",
    "print(f\"test data:\")\n",
    "predict(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finetune with dataset using huggingface trainer\n",
    "**Task:** Given a pair of sentences, detect whether the sentence is a paraphrase of another sentence    \n",
    "**Base model:** [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)  \n",
    "**Dataset:** [Glue-mrpc](https://huggingface.co/datasets/nyu-mll/glue/viewer/mrpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "<img src=\"images/bert_transfer_learning.jpeg\" alt=\"drawing\" width=\"512\"/>  \n",
    "\n",
    "Image credit: [Natural Language Processing with Transformers](.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\" \n",
    "# load the model\n",
    "class_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OW7wSFeWJtnC",
    "outputId": "3889d1b8-9505-42e0-b024-cdb36009097a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5LxKQvsjJtnD",
    "outputId": "b5196a7e-e3b5-4593-89e7-a0a2aa1ca19f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore samples using indexes just like python dictionaries\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "slRlSKb-JtnD",
    "outputId": "b047d0d4-1e38-4e4f-fdb5-75549861ae9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4nihTK98JtnD"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer can take pair of sentences and convert it into a format model requires\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "WiqlSMOqJtnE",
    "outputId": "55b3d592-51f0-4441-a112-db37b3476ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JTd8cveKJtnE"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UxdI7BzmJtnE"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Q215IiUCJtnE",
    "outputId": "7e6ed454-8dbe-4f2d-cb65-2058ca71cee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why use map\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nuLkYbAyJtnE"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZWcUsrfwJtnE",
    "outputId": "8e580f26-734e-49d4-a260-6d9a77d99a7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-i3MR4rBJtnE",
    "outputId": "86571327-084b-447a-a264-552e5d5d07c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_11441/1293272403.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/ankush/workplace/personal_projects/llm-finetuning/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "# forward pass, backpropagation, weights update all in single command\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8529411764705882, 'f1': 0.8979591836734694}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install scikit-learn\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.394863</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.890459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.444929</td>\n",
       "      <td>0.877451</td>\n",
       "      <td>0.910394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.536448</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.916376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3402927429154894, metrics={'train_runtime': 171.9463, 'train_samples_per_second': 63.997, 'train_steps_per_second': 8.008, 'total_flos': 405114969714960.0, 'train_loss': 0.3402927429154894, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "1. [BERT base model (uncased)](https://huggingface.co/google-bert/bert-base-uncased)\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Processing the data (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
