{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "mIrQ9wZmJtm9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Finetuning an encoder only model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Encoder-decoder architecture\n",
    "The encoder-decoder architecture is an ML architecture that is widely used in NLP tasks such as machine translation, text summarization, and language generation.  \n",
    "The **encoder** takes a variable-length sequence as input and transforms it into a state with a fixed shape (**thought vector**) and the **decoder** maps the encoded state of a fixed shape to an output sequence.  \n",
    "![title](images/encoder_decoder_architecture.png)  \n",
    "*Image Credits: [Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Variations of encoder-decoder architectures:\n",
    "| | Encoder-Decoder models | Encoder only models | Decoder only models |\n",
    "| --- | --- | --- | --- |\n",
    "| Analogy: |    | ![title](images/encoder_only_meme.jpg) | ![decoder only meme](images/decoder_only_meme.jpg)|\n",
    "|Use cases: | sequence to sequence tasks like machine translation, document summarization | Embedding tasks, transfer learning for downstream tasks like classification | Language generation, completion, and other generative tasks. |\n",
    "|Training objective: | Trained to minimize the difference between the predicted and target output sequences.  | Pre-trained on unsupervised tasks like language modeling, masked language modeling, etc.  | Pre-trained for generative tasks, often using autoregressive language modeling. |\n",
    "|Examples: | T5(Text-to-Text Transfer Transformer), BART   | BERT(Bidirectional Encoder Representations from Transformers) | GPT family |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## BERT model \n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- Encoder only model\n",
    "- Trains on both left as well as right context across all the layers\n",
    "- Two tasks:\n",
    "  - Masked Language modelling(MLM)\n",
    "  - Next word prediction(NSP)\n",
    "- Architecture:\n",
    "   - BERT-BASE (Layers=12, Hidden layer dimensions=768, Attention heads=12, Total Parameters=110M)\n",
    "   - BERT-LARGE (Layers=24, Hidden layer dimensions=1024, Attention heads=16, Total Parameters=340M).\n",
    "- Pretrained on Wikipedia and BooksCorpus\n",
    "- Task specific finetuning using different corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![Bert architecture](images/bert_architecture.png)\n",
    "\n",
    "Picture credits: [Bert paper](https://aclanthology.org/N19-1423.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "eihByiSVJtnA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "ZcOg2v-AJtnB",
    "outputId": "832cd899-c9bd-4e84-ae07-8da1509f0641",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate transformers[sentencepiece]\n",
    "#!pip install 'accelerate>=0.26.0'\n",
    "#!pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/google-bert/bert-base-uncased\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer (text=>tokens=>token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.vocab_size: 30522\n",
      "tokenizer.unk_token: [UNK] = 100\n",
      "zyxw: [100]\n",
      "tokenizer.sep_token: [SEP] = 102\n",
      "tokenizer.pad_token: [PAD] = 0\n",
      "tokenizer.cls_token: [CLS] = 101\n",
      "tokenizer.mask_token: [MASK]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokenizer.vocab_size: {tokenizer.vocab_size}\")\n",
    "# special tokens\n",
    "print(f\"tokenizer.unk_token: {tokenizer.unk_token} = {tokenizer.unk_token_id}\")\n",
    "print(f\"zyxw: {tokenizer.convert_tokens_to_ids(['zyxw'])}\")\n",
    "\n",
    "print(f\"tokenizer.sep_token: {tokenizer.sep_token} = {tokenizer.sep_token_id}\")\n",
    "print(f\"tokenizer.pad_token: {tokenizer.pad_token} = {tokenizer.pad_token_id}\")\n",
    "print(f\"tokenizer.cls_token: {tokenizer.cls_token} = {tokenizer.cls_token_id}\")\n",
    "print(f\"tokenizer.mask_token: {tokenizer.mask_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is used to encode and decode text.  \n",
    "encode: token => token_id  \n",
    "decode: token_id => token  \n",
    "\n",
    "\n",
    "**Vocabulary size**: number of unique tokens in the vocabulary.  \n",
    "**Special tokens**: special tokens that are used in the model.\n",
    "- UNK token: token used to represent unknown tokens.\n",
    "- SEP token: token used to separate input ids into different sequences.\n",
    "- PAD token: token used to pad sequences.\n",
    "- CLS token: token used to start sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Mask filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: The capital of the France is [MASK].\n",
      "===Tokenization===\n",
      "tokenizer output: {'input_ids': tensor([[ 101, 1996, 3007, 1997, 1996, 2605, 2003,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tokens: ['[CLS]', 'the', 'capital', 'of', 'the', 'france', 'is', '[MASK]', '.', '[SEP]']\n",
      "mask_token_index: tensor([7])\n",
      "===Prediction===\n",
      "outputs.logits.shape: torch.Size([1, 10, 30522])\n",
      "===Output===\n",
      "mask_logits: tensor([[-3.8257, -3.8663, -3.7440,  ..., -2.8305, -3.2609, -3.1235]])\n",
      "mask_probs: tensor([[1.4248e-07, 1.3680e-07, 1.5460e-07,  ..., 3.8543e-07, 2.5063e-07,\n",
      "         2.8755e-07]])\n",
      "topk_ids: [3000, 22479, 22451, 29025, 10241]\n",
      "topk_tokens: ['paris', 'lille', 'brest', 'pau', 'lyon']\n",
      "topk_probs: [0.303478479385376, 0.05363153666257858, 0.039356473833322525, 0.035300858318805695, 0.03322099521756172]\n"
     ]
    }
   ],
   "source": [
    "input_text =\"The capital of the France is [MASK].\"\n",
    "\n",
    "def fill_mask(sentence, topk=5):\n",
    "    \"\"\"\n",
    "    Print topk candidates for the masked token in the sentence.\n",
    "    \"\"\"\n",
    "    if \"[MASK]\" not in sentence:\n",
    "        raise ValueError(\"Input sentence must contain [MASK] token.\")\n",
    "\n",
    "    print(f\"sentence: {sentence}\")\n",
    "    print(f\"===Tokenization===\")\n",
    "    # Tokenize input and get tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    # input_ids, token_type_ids, attention_mask\n",
    "    # print tokens\n",
    "    print(f\"tokenizer output: {inputs}\")\n",
    "    print(f\"tokens: {tokenizer.convert_ids_to_tokens(inputs.input_ids[0])}\")\n",
    "\n",
    "    # findout the index of tokens which is masked\n",
    "    mask_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "    print(f\"mask_token_index: {mask_token_index}\")\n",
    "\n",
    "    print(f\"===Prediction===\")\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    print(f\"outputs.logits.shape: {outputs.logits.shape}\")\n",
    "\n",
    "    print(f\"===Output===\")\n",
    "    \n",
    "    # Extract the logits for the masked token\n",
    "    mask_logits = outputs.logits[0, mask_token_index, :]\n",
    "\n",
    "    print(f\"mask_logits: {mask_logits}\")\n",
    "    # convert into probabilities\n",
    "    mask_probs = torch.softmax(mask_logits, dim=1)\n",
    "\n",
    "    print(f\"mask_probs: {mask_probs}\")\n",
    "    # Get top-k tokens\n",
    "    topk_ids = torch.topk(mask_logits, topk, dim=1).indices[0].tolist()\n",
    "    \n",
    "    # topk probabilities\n",
    "    topk_probs = torch.topk(mask_probs, topk, dim=1).values[0].tolist()\n",
    "\n",
    "    \n",
    "    \n",
    "    topk_tokens = [tokenizer.decode([token_id]) for token_id in topk_ids]\n",
    "    print(f\"topk_ids: {topk_ids}\")\n",
    "    print(f\"topk_tokens: {topk_tokens}\")\n",
    "    print(f\"topk_probs: {topk_probs}\")\n",
    "\n",
    "\n",
    "fill_mask(input_text)\n",
    "# fill_mask(\"the capital of [MASK] is New Delhi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/net_loss_optimizer.png\" alt=\"drawing\" width=\"512\"/>  \n",
    "\n",
    "Image credit: [Deep learning with Python](https://www.manning.com/books/deep-learning-with-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##  Finetune with toy dataset using torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Base model:** [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)  \n",
    "**Task:** Given a review, classify it into a positive or negative class  \n",
    "**Dataset:** Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "class_model = AutoModelForSequenceClassification.from_pretrained(checkpoint) # instead of AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: ['The movie was awesome.', 'I had the worst experience of my life.']\n",
      "outputs.logits.shape: torch.Size([2, 2])\n",
      "The movie was awesome.: tensor([0.7702, 0.2298])\n",
      "I had the worst experience of my life.: tensor([0.3837, 0.6163])\n"
     ]
    }
   ],
   "source": [
    "def predict(sentences):\n",
    "    print(f\"sentence: {sentences}\")\n",
    "    # print(f\"=====TOKENIZATION=====\")\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # print(f\"inputs.input_ids:{inputs.input_ids}\")\n",
    "    # print(f\"inputs.attention_mask:{inputs.attention_mask}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = class_model(**inputs)\n",
    "        # print(f\"=====OUTPUT=====\")\n",
    "        print(f\"outputs.logits.shape: {outputs.logits.shape}\")\n",
    "        # print(f\"outputs.logits: {outputs.logits}\")\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        # print(f\"outputs.prob: {predictions}\")\n",
    "        \n",
    "    \n",
    "    for sentence, pred in zip(sentences, predictions):\n",
    "        print(f\"{sentence}: {pred}\")\n",
    "\n",
    "predict([\"The movie was awesome.\", \"I had the worst experience of my life.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: [\"I've been waiting for a HuggingFace course my whole life.\", 'This course is amazing!', 'The movie was horrible.']\n",
      "I've been waiting for a HuggingFace course my whole life.: tensor([0.5586, 0.4414])\n",
      "This course is amazing!: tensor([0.4847, 0.5153])\n",
      "The movie was horrible.: tensor([0.5291, 0.4709])\n",
      "sentence: ['The hotel was not that good.', 'I hate this so much!', 'The movie was great.']\n",
      "The hotel was not that good.: tensor([0.5151, 0.4849])\n",
      "I hate this so much!: tensor([0.5245, 0.4755])\n",
      "The movie was great.: tensor([0.5309, 0.4691])\n"
     ]
    }
   ],
   "source": [
    "training_sentences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "    \"The movie was horrible.\",\n",
    "]\n",
    "\n",
    "test_sentences = [\n",
    "    \"The hotel was not that good.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"The movie was great.\",\n",
    "]\n",
    "\n",
    "predict(training_sentences)\n",
    "predict(test_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare toy classification dataset out of above sentences\n",
    "inputs = tokenizer(training_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs[\"labels\"] = torch.tensor([1, 1, 0])\n",
    "\n",
    "\n",
    "# finetune the model\n",
    "# setup optimizer\n",
    "optimizer = torch.optim.Adam(class_model.parameters(), lr=5e-5)\n",
    "\n",
    "for _ in range(3):\n",
    "    # forward pass to calculate loss\n",
    "    loss = class_model(**inputs).loss\n",
    "    # backward pass to calculate gradients\n",
    "    loss.backward()\n",
    "    # update model weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========after finetuning======\n",
      "===training data:===\n",
      "sentence: [\"I've been waiting for a HuggingFace course my whole life.\", 'This course is amazing!', 'The movie was horrible.']\n",
      "I've been waiting for a HuggingFace course my whole life.: tensor([0.2933, 0.7067])\n",
      "This course is amazing!: tensor([0.3004, 0.6996])\n",
      "The movie was horrible.: tensor([0.8286, 0.1714])\n",
      "===test data:===\n",
      "sentence: ['The hotel was not that good.', 'I hate this so much!', 'The movie was great.']\n",
      "The hotel was not that good.: tensor([0.5186, 0.4814])\n",
      "I hate this so much!: tensor([0.3166, 0.6834])\n",
      "The movie was great.: tensor([0.7724, 0.2276])\n"
     ]
    }
   ],
   "source": [
    "print(f\"=========after finetuning======\")\n",
    "print(f\"===training data:===\")\n",
    "predict(training_sentences)\n",
    "\n",
    "print(f\"===test data:===\")\n",
    "predict(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "<img src=\"images/bert_transfer_learning.jpeg\" alt=\"drawing\" width=\"512\"/>  \n",
    "\n",
    "Image credit: [Natural Language Processing with Transformers](.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finetune with dataset using huggingface trainer\n",
    "**Task:** Given a pair of sentences, detect whether the sentence is a paraphrase of another sentence    \n",
    "**Base model:** [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)  \n",
    "**Dataset:** [Glue-mrpc](https://huggingface.co/datasets/nyu-mll/glue/viewer/mrpc) The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems. MRPC is specific to paraphrasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\" \n",
    "# load the model\n",
    "class_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OW7wSFeWJtnC",
    "outputId": "3889d1b8-9505-42e0-b024-cdb36009097a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# \n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5LxKQvsjJtnD",
    "outputId": "b5196a7e-e3b5-4593-89e7-a0a2aa1ca19f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore samples using indexes just like python dictionaries\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "slRlSKb-JtnD",
    "outputId": "b047d0d4-1e38-4e4f-fdb5-75549861ae9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4nihTK98JtnD"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  tokenize sentence 1, sentence 2 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 27, 23, 41, 30, 21, 31, 15]\n",
      "[26, 33, 25, 27, 30, 30, 32, 18]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "print([len(sample) for sample in tokenized_sentences_1.input_ids[:8]])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "print([len(sample) for sample in tokenized_sentences_2.input_ids[:8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize pair as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer can take pair of sentences and convert it into a format model requires\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(f\"inputs:{inputs}\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with fix padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "UxdI7BzmJtnE"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Q215IiUCJtnE",
    "outputId": "7e6ed454-8dbe-4f2d-cb65-2058ca71cee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "for step, batch in enumerate(train_data_loader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toeknize with dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "nuLkYbAyJtnE"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ea03c2213c4797a93409d7f0aefab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 67])\n",
      "torch.Size([16, 79])\n",
      "torch.Size([16, 73])\n",
      "torch.Size([16, 71])\n",
      "torch.Size([16, 78])\n",
      "torch.Size([16, 70])\n",
      "torch.Size([16, 79])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_data_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, collate_fn=data_collator )\n",
    "for step, batch in enumerate(train_data_loader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.450421</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>0.457906</td>\n",
       "      <td>0.850490</td>\n",
       "      <td>0.892416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.626026</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.905009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.3633915188525497, metrics={'train_runtime': 178.3067, 'train_samples_per_second': 61.714, 'train_steps_per_second': 7.723, 'total_flos': 405114969714960.0, 'train_loss': 0.3633915188525497, 'epoch': 3.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup training using huggingface accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage GPU acceleration without accelerate library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ankush/workplace/personal_projects/llm-finetuning/venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becf016824ae4230a910e7ab9d534576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.optim import AdamW \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# pick GPU device if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # move batch to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leverage GPU acceleration without accelerate library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ankush/workplace/personal_projects/llm-finetuning/venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fa05e1f4c546d1a98fa541537b6995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8799019607843137, 'f1': 0.9153713298791019}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dl:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model on local/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e355b4375344037aea82b84ecd8fce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Ankush-Chander/test-trainer/commit/0bc5396d680632cc1f6fa7829ec560c915580891', commit_message='Upload BertForSequenceClassification', commit_description='', oid='0bc5396d680632cc1f6fa7829ec560c915580891', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Ankush-Chander/test-trainer', endpoint='https://huggingface.co', repo_type='model', repo_id='Ankush-Chander/test-trainer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"test-trainer\")\n",
    "\n",
    "# save model on huggingface hub\n",
    "model.push_to_hub(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# References:\n",
    "1. [illustrated-transformer](https://jalammar.github.io/illustrated-transformer)\n",
    "2. [Hugginface NLP Course](https://huggingface.co/learn/nlp-course)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Processing the data (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
