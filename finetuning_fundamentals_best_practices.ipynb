{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87337d0-fc81-43dc-b502-09f1a9e3c50f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Finetuning: Fundamentals and best practices\n",
    "\n",
    "Ankush Chander  \n",
    "Dhirubhai Ambani University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78632d24-41c4-4dec-8241-669345cba932",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LLM overview\n",
    "### Language modelling\n",
    "Language modeling is a task to predict the next word or character in a sequence of text given the context of the previous words.\n",
    "\n",
    "$$\n",
    "P(w_n|w_1, w_2, ..., w_n-1) = ?\n",
    "$$\n",
    "\n",
    "<img src=\"images/next_word_pred.jpeg\" width=400 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb711953-156f-45b8-832f-cd7d33170abc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Evolution of Language models\n",
    "| | Statistical Language models | Neural Language models | Large<sup>[1]</sup> Language models |\n",
    "| --- | --- | --- | --- |\n",
    "|Pros: | Simple to implement | generalizes well to unseen sequences as it capture semantic relationships | generate coherent and contextually relevant text. |\n",
    "|Cons: | 1.struggle with capturing long-range dependencies. <br> 2. didn\"t capture semantic relationships. <br>Eg: cat sat on a table. vs cat sat on a desk  | Expensive to train | Expensive to train |\n",
    "|Examples: | N-gram models, Hidden Markov Models (HMMs).   | RNNs, LSTMs, GRU | GPT-3, GPT-4, BERT |\n",
    "\n",
    "\n",
    "1- **Large** Number of parameters(variables that gets updated while training a model) + amount of data on which they are trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d734a40-8bd9-4a4a-92bf-cec18de2f7ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Stages of LLM training\n",
    "\n",
    "<img src=\"images/llm_training_stages.jpeg\" width=800 />\n",
    "\n",
    "Image credits: [AI Engineering by Chip Huyen](https://github.com/chiphuyen/aie-book)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b82197-10db-450f-83fe-8a7d6f90c28b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Different ways of applying LLMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac59bf-a45f-4238-a179-4c6526b69315",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Prompt Engineering\n",
    "Prompt Engineering refers to the process of crafting an instruction that gets model to generate the desired outcome.  \n",
    "**Pros:**\n",
    "- Quick and easy to implement.\n",
    "- No additional training required.\n",
    "- Low computational cost.\n",
    "- Works with any pre-trained model.\n",
    "\n",
    "**Cons:**\n",
    "- Limited customization.\n",
    "- Relies on model’s existing knowledge.\n",
    "- Hard to handle specialized or domain-specific tasks.\n",
    "- Prone to hallucinations if model lacks relevant data.\n",
    " \n",
    "**Key terms:**  \n",
    "_In-context learning:_ Models ability to answer questions based on context provided in prompt without explicit training.    \n",
    "_Few shot learning:_ PRoviding examples to teach model how to answer    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7c69e-062a-4ef6-9e97-5dfbdae3d79c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Retrieval Augmented Generation(RAG)\n",
    "RAG enhances model responses by *retrieving relevant external data* and providing it as context.\n",
    "\n",
    "Pros:\n",
    "- Reduces hallucination by grounding responses in retrieved documents.\n",
    "- No need to modify model weights.\n",
    "- Can adapt to dynamic or evolving data\n",
    "\n",
    "Cons: \n",
    "- Increased latency due to retrieval step.\n",
    "- Performance depends on quality of retrieved documents.\n",
    "- Requires infrastructure for storing and indexing documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdadc0-bb26-4062-aa8e-fd61af6c79cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Finetuning\n",
    "Finetuning is a process of adapting a model to a specific task by updating whole or a part of the model.  \n",
    "\n",
    "Example use cases: \n",
    "- Enhance model\"s domain specific abilities like code completion, medical question answering\n",
    "- strenghten safety  \n",
    "- improve instruction following ability  \n",
    "\n",
    "Pros: \n",
    "- Highly customizable\n",
    "- Allows adaptation to new domains, styles, or behaviors.\n",
    "\n",
    "Cons:\n",
    "- Requires high upfront investment and continual maintainance\n",
    "- Resource intensive\n",
    "- Harder to update dynamically compared to RAG.\n",
    "- Requires a well-labeled dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f50d0-66f8-45b6-88a9-0d379b92b7d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Development workflow\n",
    "<img src=\"images/dev_workflow.jpeg\" width=800 />\n",
    "\n",
    "Image credits: [AI Engineering by Chip Huyen](https://github.com/chiphuyen/aie-book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a6efb-e0cd-410b-949b-7343e630f166",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Demo 1: Fine tune an encoder only model using hugginface\n",
    "<img src=\"images/qrcode_encoder_only.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1b743-2da3-40cd-ab12-81f587ca7f63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Beginner guide to finetuning an opensource LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7476748-b3b4-4ef7-9f3f-81cc177b523b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Hardware estimation\n",
    "\n",
    "<img src=\"images/net_loss_optimizer.png\" width=500>\n",
    "\n",
    "$$Model memory = model\\_weights+ gradients + activations + optimizer\\_states$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99657809-90a1-42f5-b25a-eaf787b79f2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### During Inference\n",
    "\\begin{align*}\\text{Total Memory}_{\\text{Training}} = \\text{Model Memory}+\\text{Activation Memory}\\end{align*}\n",
    "\n",
    "- **Forward Pass**: Only requires memory for the model's weights.\n",
    "  - Memory for weights: `N x M`\n",
    "    - `N`: Model's parameter count\n",
    "    - `M`: Memory needed for each parameter\n",
    "- **Additional Memory**: Required for activation and key-value vectors in transformer models.\n",
    "  - Assumed to be 20% of the model's weights.\n",
    "  - Total memory footprint: `N x M x 1.2`\n",
    "- **Example Calculation**:\n",
    "  - 13B-parameter model, 2 bytes/parameter\n",
    "  - Weights = 26 GB\n",
    "  - Total Inference Memory = 31.2 GB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32785a93-7305-415b-9b59-3005b8baf6c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### During Training\n",
    "- **Overall Memory Needs**: \n",
    "\\begin{align*}\\text{Total Memory}_{\\text{Training}} = \\text{Model Memory}+\\text{Optimiser Memory}+\\text{Activation Memory}+\\text{Gradient Memory}\\end{align*}\n",
    "\n",
    "- **Backward Pass**:\n",
    "  - Each trainable parameter may need:\n",
    "    - Gradient value\n",
    "    - Optimizer states (depending on optimizer type)\n",
    "  - **Optimizers**:\n",
    "    - Vanilla SGD: No state\n",
    "    - Momentum: One value per parameter\n",
    "    - Adam: Two values per parameter\n",
    "\n",
    "- **Example Calculation**:\n",
    "  - 13B-parameter model, Adam optimizer\n",
    "  - Each parameter has 3 values (gradient + optimizer states)\n",
    "      - Memory for gradients and optimizer states =  13 * 2 *(1+2)=  78 GB\n",
    "      - Total memory = 13 * 2 * (1+1+2) = 104GB\n",
    "  - \n",
    "\n",
    "- **Activation Memory**:\n",
    "  - Can surpass memory needed for weights.\n",
    "  - **Optimization**: Use gradient checkpointing to reduce memory, at the cost of increased training time.\n",
    "\n",
    "- **Additional Notes**:\n",
    "  - Memory needs grow rapidly with model size.\n",
    "  - Techniques to reduce memory consumption include recomputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b0069-1e1e-4267-8a80-135dae69b0cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "`accelerate-estimate-memory` is a CLI command that will load the model into memory on the meta device, so we are not actually downloading and loading the full weights of the model into memory, nor do we need to. As a result it’s perfectly fine to measure 8 billion parameter models (or more), without having to worry about if your CPU can handle it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13834394-3a7a-4538-8c53-feb0d1f38a32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `microsoft/Phi-3.5-mini-instruct` from `transformers`...\n",
      "┌────────────────────────────────────────────────────────────┐\n",
      "│ Memory Usage for loading `microsoft/Phi-3.5-mini-instruct` │\n",
      "├───────┬─────────────┬──────────┬───────────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│    Training using Adam    │\n",
      "├───────┼─────────────┼──────────┼───────────────────────────┤\n",
      "│float32│  432.02 MB  │ 14.23 GB │          56.94 GB         │\n",
      "│float16│  216.01 MB  │ 7.12 GB  │          28.47 GB         │\n",
      "│  int8 │  108.01 MB  │ 3.56 GB  │            N/A            │\n",
      "│  int4 │   54.0 MB   │ 1.78 GB  │            N/A            │\n",
      "└───────┴─────────────┴──────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
    "!accelerate-estimate-memory microsoft/Phi-3.5-mini-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecddc869-17c7-4c6a-bc60-7d0760afc3fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-13b-hf` from `transformers`...\n",
      "config.json: 100%|█████████████████████████████| 610/610 [00:00<00:00, 5.51MB/s]\n",
      "┌────────────────────────────────────────────────────────┐\n",
      "│  Memory Usage for loading `meta-llama/Llama-2-13b-hf`  │\n",
      "├───────┬─────────────┬──────────┬───────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│  Training using Adam  │\n",
      "├───────┼─────────────┼──────────┼───────────────────────┤\n",
      "│float32│   1.18 GB   │ 47.88 GB │       191.51 GB       │\n",
      "│float16│  605.02 MB  │ 23.94 GB │        95.76 GB       │\n",
      "│  int8 │  302.51 MB  │ 11.97 GB │          N/A          │\n",
      "│  int4 │  151.25 MB  │ 5.98 GB  │          N/A          │\n",
      "└───────┴─────────────┴──────────┴───────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!accelerate-estimate-memory meta-llama/Llama-2-13b-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b417c-e988-41bf-8457-dfaf0442df31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Finetuning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717fbfd-68c1-435e-b13f-65599c75a3f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Quantization\n",
    "Model quantization is a common way to reduce model hardware requirements. Reducing the precision of the model weights and activations of the model reduces the GPU RAM requirements. For example changing model precision from float16 to int8 halves the size of the VRAM requirements.\n",
    "It also leads to kv cache size reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bfe69-8796-4658-955e-20737d8529a2",
   "metadata": {},
   "source": [
    "##### Floating point representation\n",
    "\n",
    "| Representation | Mantissa     | Exponent(range of numbers)                          | Sign | Example   |\n",
    "| -------------- | ------------ | --------------------------------------------------- | ---- | --------- |\n",
    "|                | decides the precision with which numbers can be represented | decides the range of number that can be represented |      |           |\n",
    "| FP32           | 23           | 8                                                   | 1    | 3.1415927 |\n",
    "| FP16           | 10           | 5                                                   | 1    | 3.141     |\n",
    "| FP8            | 2            | 5                                                   | 1    | 3         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8c833-5629-4c55-a073-3a23166b5827",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Adapter based techniques\n",
    "LoRA, short for Low-Rank Adaptation, is a method designed to efficiently fine-tune large pre-trained models. The intuition behind LoRA stems from the understanding that the vast majority of the parameters in a pre-trained model remain unchanged during fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d8d34-b315-4ff6-9737-4f34fb26bc5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![Title](images/lora_diagram.png)\n",
    "Image credits: [huggingface](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476afdb-a91d-4545-a278-1090abe0b03e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "3. [Hardware for LLMs - by Benjamin Marie](https://kaitchup.substack.com/p/hardware-for-llms)\n",
    "4. [Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX](https://www.usenix.org/conference/osdi22/presentation/yu)\n",
    "5. [QLoRA: Fine-Tune a Large Language Model on Your GPU](https://kaitchup.substack.com/p/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)\n",
    "6. [Fundamentals of Data Representation: Floating point numbers - Wikibooks, open books for an open world](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers)\n",
    "7. [AI engineering resources](https://github.com/chiphuyen/aie-book/blob/main/resources.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
